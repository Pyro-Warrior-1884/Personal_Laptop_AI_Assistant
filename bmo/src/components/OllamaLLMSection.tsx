import "./OllamaLLMSection.css"

export default function OllamaLLMSection() {
  return (
    <div class="section-text">
      <h2 class="section-title">Ollama LLM</h2>
      <p class="section-paragraph">
        At the heart of BMO lies Ollama, a powerful framework for running large language models locally. Ollama provides efficient model management, allowing us to deploy models like Llama, Mistral, and others with minimal overhead. The integration enables real-time inference with optimized memory usage, supporting various model sizes depending on hardware capabilities. This approach ensures that BMO can deliver intelligent responses while maintaining complete control over the AI infrastructure and user data.
      </p>
    </div>
  );
}