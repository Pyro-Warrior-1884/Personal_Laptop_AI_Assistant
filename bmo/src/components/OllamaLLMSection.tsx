import "./OllamaLLMSection.css";

export default function OllamaLLMSection() {
  return (
    <div class="section-text">
      <h2 class="section-title">Ollama LLM</h2>

      {/* Overview */}
      <p class="section-paragraph">
        Ollama serves as the core engine responsible for all local language model
        inference within the BMO system. It provides a dedicated runtime designed
        to load, manage, and optimize large language models directly on the host
        machine. This enables the assistant to perform natural language
        processing entirely offline, ensuring data privacy and independence
        from cloud-based services.
      </p>

      {/* Integration with Backend */}
      <h3 class="section-subtitle">LLM Integration</h3>
      <p class="section-paragraph">
        The Python backend communicates with Ollama using a lightweight
        subprocess-based execution model. User prompts are passed directly to
        the <strong>bmo</strong> language model, and generated responses are
        captured from standard output in real time. Timeouts and structured
        error handling mechanisms are implemented to prevent blocking behavior
        and ensure overall system stability.
      </p>

      {/* Offline Inference */}
      <p class="section-paragraph">
        All responses generated by the Ollama runtime are processed entirely
        offline. When no predefined or custom command matches a user request,
        the assistant automatically falls back to the local LLM to generate an
        intelligent, context-aware response.
      </p>

      {/* Benefits */}
      <h3 class="section-subtitle">Key Advantages</h3>
      <ul class="section-list">
        <li>Complete data privacy through local inference</li>
        <li>Zero dependency on cloud-based language models</li>
        <li>Fast response times with minimal latency</li>
        <li>Predictable performance and hardware-level control</li>
      </ul>

      {/* Performance & Scalability */}
      <p class="section-paragraph">
        Ollama also manages essential model lifecycle operations such as model
        loading, unloading, caching, and request handling. These optimizations
        reduce inference latency and memory overhead, allowing continuous and
        responsive interaction between the speech recognition pipeline and the
        response generation layer. This architecture enables future model
        upgrades or replacements without requiring changes to higher-level
        application logic.
      </p>
    </div>
  );
}
